---
title: "Manuscript Unformatted"
author: "Craig McGowan"
date: "November 16, 2018"
output: pdf_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(FluSight)
library(cdcfluview)
library(pander)

knitr::opts_chunk$set(echo = FALSE)
knitr::opts_knit$set(root.dir = "../")
```

# Abstract

# Intro

* Flu is bad - here's why

* Why forecast flu

* Summary of other methodologies

* CDC challenge

* What this paper does

# Methods

## Forecast structure
Weekly forecasts were structured according to the template used by the FluSight challenge (Biggerstaff et al 2018, McGowan et al 2019). Within each week, forecasts were generated for each of seven targets in eleven locations. Forecast locations included the United States as a whole and each of the 10 Health and Human Services Regions, which each encompasses several US states. Forecast targets could be generally grouped into seasonal targets and short-term targets. Seasonal targets included season peak percentage, defined as the maximum wILI% value, season peak week, defined as the week when the maximum wILI% occurred, and season onset, defined as the first week where the wILI% value matched or exceed the location-specific baseline value and remained at or above for three consecutive weeks. Short-term targets included forecasts of wILI% values 1, 2, 3, and 4 weeks ahead of the most recently published data. Possible outcomes for each target are grouped into bins, and the forecasts assign a probability of occurrance to each possible outcome for each target. (Table 1) 

## Model definition and terminology
Let $y_{l,t}$ be the observed wILI% value for location $l$ at MMWR week $t$. Models for all locations were of the general form

\begin{equation}
g_l\left(y_{l,t}\right) = a_l + \sum_{k=1}^{K_l}\left[\alpha_{k1} \text{sin}\left(\frac{2\pi kt}{52}\right) + \alpha_{k2} \text{cos}\left(\frac{2\pi kt}{52}\right)\right] + \boldsymbol{\beta}_l \boldsymbol{X}_{l,t} + N_{l,t}
\end{equation}

where $g_l()$ is a location-specific link function, $K_l$ is a location-specific number of Fourier terms, $\boldsymbol{X}_{l,t}$ is a matrix of observed covariates for location $l$, and $N_{l}$ is an ARIMA error process. Separate models were fit for each forecast location, with the link function, number of Fourier terms, ARIMA error structure, and included covariates allowed to vary between locations. 

## Model fitting
Models for each location were fit using a sequential cross-validation process in `R` using tools from the `forecast` package. The link function $g_l()$ was chosen first, followed by the number of Fourier terms $K_l$, the ARIMA error structure $N_{l}$, and covariates $\boldsymbol{X}_l$. Possible choices for $g_l()$, $K_l$, $N_l$, and $\boldsymbol{X}_l$ are summarized in Table \ref{table:compopt}. Training data consisted of the 2004/2005 through 2013-2014 influenza seasons, excluding the 2008/2009 and 2009/2010 seasons where typical seasonal patterns were disrupted by the H1N1 pandemic. For models of later test seasons, data from test seasons that had already occurred were included in the training data (e.g. 2015/2016 for models of 2016/2017). 

\input{"static-content/cv_options.tex"}

To determine $g_l()$ for a particular test season, we used data from 2010/2011 season through the prior to that test season as cross-validation seasons. For each CV season, we fit a location-specific SARIMA model in `R` using the `auto.arima` function from the `forecast` package for each possible link function $g_l()$ to the training data prior to the CV season. We generated weekly probabilistic forecasts for the CV season by sampling from the predicted trajectory using bootstrapped errors. Forecasts were scored using the scoring rules for the FluSight challenge (see 'Scoring Rules' below) and averaged across all CV seasons. The link function with the highest average score across the CV seasons was selected for each location. Values and/or structures for $K_l$, $N_l$, and $X_l$ were determined sequentially in an analogous fashion, incorporating decisions made for prior model components (e.g. CV models to determine $N_l$ incorporated previously calculated values for $g_l()$ and $K_l$).

We considered several potential covariates for inclusion in the test model for each location, including cumulative virus subtype percentage, national Google Trends (GT), regional GT, and observed revisions to initially published ILINet values. 

## Development of 'prospective' forecasts

## Scoring rules

# Results

```{r load scores}
scores <- bind_rows(
  readRDS("Data/histavg_scores.Rds"),
  readRDS("Data/sarima_scores.Rds") %>%
    mutate(model = "SARIMA"),
  bind_rows(
    readRDS("Data/prospective_scores_1415.Rds"),
    readRDS("Data/prospective_scores_1516.Rds")#,
    # readRDS("Data/prospective_scores_1617.Rds"),
    # readRDS("Data/prospective_scores_1718.Rds")
  ) %>%
    mutate(model = "Springbok")
) %>%
  mutate(target_type = ifelse(target %in% c("Season onset", "Season peak percentage",
                                            "Season peak week"),
                              "Seasonal", "Short-term"))
```


* Summary results from each test season


```{r overall scores across seasons}
scores %>%
  group_by(season, model) %>%
  summarize(mean_skill = exp(mean(score))) %>%
  spread(key = "model", value = "mean_skill") %>%
  pander()
```

* Comparisons across seasons and locations and targets

```{r target season comparison}

loc_tt_scores <- scores %>%
  group_by(model, location, target_type) %>%
  summarize(mean_skill = exp(mean(score))) %>%
  ungroup() %>%
  left_join(., filter(., model == "Historical Average") %>%
              select(hist_skill = mean_skill, target_type, location),
            by = c("location", "target_type")) %>%
  mutate(diff = mean_skill - hist_skill)


ggplot(loc_tt_scores, aes(x = location, y = model)) + 
  geom_tile(aes(fill = diff)) +
  geom_text(aes(label = round(mean_skill, 2))) +
  scale_fill_gradient2(low = "tomato", mid = "white", high = "steelblue",
                      midpoint = 0, 
                      limits = c(-max(abs(min(loc_tt_scores$diff)),
                                      max(loc_tt_scores$diff)),
                                 max(abs(min(loc_tt_scores$diff)),
                                     max(loc_tt_scores$diff)))) +
  facet_wrap(~target_type, nrow = 2) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(x = "", y = "", fill = "Diff. from\nHist. Avg.")
```



* Compare to seasonal ARIMA forecast and historical average - illustrate improvement over baseline model

* Comparisons using MAE?

* Investigate calibration using tools from CDC paper?

```{r load forecasts}
source("R/forecast_calibration.R")

nested_truth <- readRDS("Data/nested_truth_1415_1718.Rds") 

truth <- nested_truth %>%
  select(season, truth) %>%
  unnest() %>%
  mutate(true = 1)
# 
# exp_truth <- nested_truth %>%
#   select(season, exp_truth) %>%
#   unnest() %>%
#   mutate(true = 1)

all_forecasts <- bind_rows(
  mutate(readRDS("Data/prospective_forecasts_1415.Rds"), 
         season = "2014/2015",
         team = "DH"),
  mutate(readRDS("Data/prospective_forecasts_1516.Rds"),
         season = "2015/2016",
         team = "DH")#,
  # mutate(readRDS("Data/prospective_forecasts_1617.Rds"), 
  #        season = "2016/2017"),
  # mutate(readRDS("Data/prospective_forecasts_1718.Rds"),
  #        season = "2018/2018"))
) %>%
  unnest() %>%
  bind_rows(mutate(readRDS("Data/sarima_forecasts.Rds"),
                   team = "SARIMA") %>%
              unnest()) %>%
  bind_rows(mutate(readRDS("Data/hist_avg_forecasts.Rds"),
                   team = "Hist Avg",
                   week = case_when(forecast_week < 40 & season == "2014/2015" ~ 
                                      forecast_week + 53,
                                    forecast_week < 40 ~ forecast_week + 52,
                                    TRUE ~ forecast_week))) %>%
  mutate(challenge = "Influenza")

eval_forecasts <- all_forecasts %>%
  # Attach evaluation period and filter to only forecasts in eval period
  left_join(select(nested_truth, season, eval_period) %>%
              unnest(),
            by = c("season", "target", "location")) %>%
  filter(week >= start_week, week <= end_week) %>%
  # Attach truth and create indicator
  left_join(filter(truth, target %in% c("Season onset", "Season peak week",
                                        "Season peak percentage")) %>%
              select(-forecast_week),
            by = c("season", "location", "target", "bin_start_incl")) %>%
  left_join(filter(truth, !target %in% c("Season onset", "Season peak week",
                                        "Season peak percentage")),
            by = c("season", "location", "target", "forecast_week", "bin_start_incl")) %>%
  mutate(true = case_when(true.x == 1 | true.y == 1  ~ 1,
                          TRUE ~ 0)) %>%
  select(-true.x, -true.y)

# eval_forecasts_exp <- all_forecasts %>%
#   # Attach evaluation period and filter to only forecasts in eval period
#   left_join(select(nested_truth, season, eval_period) %>%
#               unnest(),
#             by = c("season", "target", "location")) %>%
#   filter(week >= start_week, week <= end_week) %>%
#   # Attach truth and create indicator
#   left_join(filter(exp_truth, target %in% c("Season onset", "Season peak week",
#                                         "Season peak percentage")) %>%
#               select(-forecast_week),
#             by = c("season", "location", "target", "bin_start_incl")) %>%
#   left_join(filter(exp_truth, !target %in% c("Season onset", "Season peak week",
#                                         "Season peak percentage")),
#             by = c("season", "location", "target", "forecast_week", "bin_start_incl")) %>%
#   mutate(true = case_when(true.x == 1 | true.y == 1  ~ 1,
#                           TRUE ~ 0)) %>%
#   select(-true.x, -true.y)
#   
  
```


```{r calibration plots}
forecast_calibration(eval_forecasts, percent_bin = 0.1, ssn_group = TRUE,
                     tgt_group = TRUE) %>%
  ggplot() + 
  geom_point(aes(x = mean_pred_prob, y = percent),
               size = 1.4) +
  geom_errorbar(aes(x = mean_pred_prob, ymin = lower, ymax = upper,
                    width = width/4)) +
  # Add reference line
  geom_abline(slope = 1, intercept = 0, linetype = 2) +
  # Make graph pretty
  scale_x_continuous(limits = c(-0.02, 1.02), breaks = c(0, 0.5, 1)) + 
  scale_y_continuous(limits = c(0, 1)) +
  labs(x = "Predicted probability",
       y = "Observed probability",
       color = "Season") +
  theme_minimal() +
  facet_grid(target ~ season)


```

* Investigate empirical coverage of 80% prediction intervals?

```{r}
eval_forecasts %>%
  group_by(season, team, location, target, week) %>%
  filter(cumsum(value) >= 0.1, cumsum(value) <= 0.9) %>%
  summarize(encompass_true = any(true == 1)) %>%
  ungroup() %>%
  group_by(season, team, target) %>%
  summarize(mean(encompass_true))

```

Need to fix empirical comparisons for Historical Average

* Comparisons to CDC results using relevant scoring rules

# Discussion

* New modeling approach compared to previous literature

* Strong performance compared to existing models

* Submitted as true prospective model during 18/19 season and can evaluate further then (slash maybe wait until season is over)

* Hopefully fairly strong performance independent of scoring metric, as well as decent calibration

* Strengths - fairly simple mathematical model, easy to implement with existing software

* Limitations - current analysis assumes Google Trend data are free of backfill, relies on provided sample of GT data. Assumes virologic data are free of backfill which we know not to be true, but using cumulative percentage should minimize that (and can investigate truth of that statement following 18/19 season)